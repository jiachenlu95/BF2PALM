{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6333b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc749fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6693a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interact with the OSM APIs.\"\"\"\n",
    "\n",
    "import datetime as dt\n",
    "import json\n",
    "import logging as lg\n",
    "import re\n",
    "import socket\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from hashlib import sha1\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from dateutil import parser as date_parser\n",
    "\n",
    "from . import projection\n",
    "from . import settings\n",
    "from . import utils\n",
    "from . import utils_geo\n",
    "from ._errors import CacheOnlyModeInterrupt\n",
    "\n",
    "# capture getaddrinfo function to use original later after mutating it\n",
    "_original_getaddrinfo = socket.getaddrinfo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec6261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fde8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb351a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_osm_filter(network_type):\n",
    "    \"\"\"\n",
    "    Create a filter to query OSM for the specified network type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_type : string {\"all_private\", \"all\", \"bike\", \"drive\", \"drive_service\", \"walk\"}\n",
    "        what type of street network to get\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "    \"\"\"\n",
    "    # define built-in queries to send to the API. specifying way[\"highway\"]\n",
    "    # means that all ways returned must have a highway tag. the filters then\n",
    "    # remove ways by tag/value.\n",
    "    filters = dict()\n",
    "\n",
    "    # driving: filter out un-drivable roads, service roads, private ways, and\n",
    "    # anything specifying motor=no. also filter out any non-service roads that\n",
    "    # are tagged as providing certain services\n",
    "    filters[\"drive\"] = (\n",
    "        f'[\"highway\"][\"area\"!~\"yes\"]{settings.default_access}'\n",
    "        f'[\"highway\"!~\"abandoned|bridleway|bus_guideway|construction|corridor|cycleway|elevator|'\n",
    "        f\"escalator|footway|path|pedestrian|planned|platform|proposed|raceway|service|\"\n",
    "        f'steps|track\"]'\n",
    "        f'[\"motor_vehicle\"!~\"no\"][\"motorcar\"!~\"no\"]'\n",
    "        f'[\"service\"!~\"alley|driveway|emergency_access|parking|parking_aisle|private\"]'\n",
    "    )\n",
    "\n",
    "    # drive+service: allow ways tagged 'service' but filter out certain types\n",
    "    filters[\"drive_service\"] = (\n",
    "        f'[\"highway\"][\"area\"!~\"yes\"]{settings.default_access}'\n",
    "        f'[\"highway\"!~\"abandoned|bridleway|bus_guideway|construction|corridor|cycleway|elevator|'\n",
    "        f'escalator|footway|path|pedestrian|planned|platform|proposed|raceway|steps|track\"]'\n",
    "        f'[\"motor_vehicle\"!~\"no\"][\"motorcar\"!~\"no\"]'\n",
    "        f'[\"service\"!~\"emergency_access|parking|parking_aisle|private\"]'\n",
    "    )\n",
    "\n",
    "    # walking: filter out cycle ways, motor ways, private ways, and anything\n",
    "    # specifying foot=no. allow service roads, permitting things like parking\n",
    "    # lot lanes, alleys, etc that you *can* walk on even if they're not\n",
    "    # exactly pleasant walks. some cycleways may allow pedestrians, but this\n",
    "    # filter ignores such cycleways.\n",
    "    filters[\"walk\"] = (\n",
    "        f'[\"highway\"][\"area\"!~\"yes\"]{settings.default_access}'\n",
    "        f'[\"highway\"!~\"abandoned|bus_guideway|construction|cycleway|motor|planned|platform|'\n",
    "        f'proposed|raceway\"]'\n",
    "        f'[\"foot\"!~\"no\"][\"service\"!~\"private\"]'\n",
    "    )\n",
    "\n",
    "    # biking: filter out foot ways, motor ways, private ways, and anything\n",
    "    # specifying biking=no\n",
    "    filters[\"bike\"] = (\n",
    "        f'[\"highway\"][\"area\"!~\"yes\"]{settings.default_access}'\n",
    "        f'[\"highway\"!~\"abandoned|bus_guideway|construction|corridor|elevator|escalator|footway|'\n",
    "        f'motor|planned|platform|proposed|raceway|steps\"]'\n",
    "        f'[\"bicycle\"!~\"no\"][\"service\"!~\"private\"]'\n",
    "    )\n",
    "\n",
    "    # to download all ways, just filter out everything not currently in use or\n",
    "    # that is private-access only\n",
    "    filters[\"all\"] = (\n",
    "        f'[\"highway\"][\"area\"!~\"yes\"]{settings.default_access}'\n",
    "        f'[\"highway\"!~\"abandoned|construction|planned|platform|proposed|raceway\"]'\n",
    "        f'[\"service\"!~\"private\"]'\n",
    "    )\n",
    "\n",
    "    # to download all ways, including private-access ones, just filter out\n",
    "    # everything not currently in use\n",
    "    filters[\n",
    "        \"all_private\"\n",
    "    ] = '[\"highway\"][\"area\"!~\"yes\"][\"highway\"!~\"abandoned|construction|planned|platform|proposed|raceway\"]'\n",
    "\n",
    "    if network_type in filters:\n",
    "        osm_filter = filters[network_type]\n",
    "    else:  # pragma: no cover\n",
    "        raise ValueError(f'Unrecognized network_type \"{network_type}\"')\n",
    "\n",
    "    return osm_filter\n",
    "\n",
    "\n",
    "def _save_to_cache(url, response_json, sc):\n",
    "    \"\"\"\n",
    "    Save a HTTP response JSON object to a file in the cache folder.\n",
    "\n",
    "    Function calculates the checksum of url to generate the cache file's name.\n",
    "    If the request was sent to server via POST instead of GET, then URL should\n",
    "    be a GET-style representation of request. Response is only saved to a\n",
    "    cache file if settings.use_cache is True, response_json is not None, and\n",
    "    sc = 200.\n",
    "\n",
    "    Users should always pass OrderedDicts instead of dicts of parameters into\n",
    "    request functions, so the parameters remain in the same order each time,\n",
    "    producing the same URL string, and thus the same hash. Otherwise the cache\n",
    "    will eventually contain multiple saved responses for the same request\n",
    "    because the URL's parameters appeared in a different order each time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        the URL of the request\n",
    "    response_json : dict\n",
    "        the JSON response\n",
    "    sc : int\n",
    "        the response's HTTP status code\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    if settings.use_cache:\n",
    "\n",
    "        if sc != 200:\n",
    "            utils.log(f\"Did not save to cache because status code is {sc}\")\n",
    "\n",
    "        elif response_json is None:\n",
    "            utils.log(\"Did not save to cache because response_json is None\")\n",
    "\n",
    "        else:\n",
    "            # create the folder on the disk if it doesn't already exist\n",
    "            cache_folder = Path(settings.cache_folder)\n",
    "            cache_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # hash the url to make the filename succinct but unique\n",
    "            # sha1 digest is 160 bits = 20 bytes = 40 hexadecimal characters\n",
    "            filename = sha1(url.encode(\"utf-8\")).hexdigest() + \".json\"\n",
    "            cache_filepath = cache_folder / filename\n",
    "\n",
    "            # dump to json, and save to file\n",
    "            cache_filepath.write_text(json.dumps(response_json), encoding=\"utf-8\")\n",
    "            utils.log(f'Saved response to cache file \"{cache_filepath}\"')\n",
    "\n",
    "\n",
    "def _url_in_cache(url):\n",
    "    \"\"\"\n",
    "    Determine if a URL's response exists in the cache.\n",
    "\n",
    "    Calculates the checksum of url to determine the cache file's name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        the URL to look for in the cache\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filepath : pathlib.Path\n",
    "        path to cached response for url if it exists, otherwise None\n",
    "    \"\"\"\n",
    "    # hash the url to generate the cache filename\n",
    "    filename = sha1(url.encode(\"utf-8\")).hexdigest() + \".json\"\n",
    "    filepath = Path(settings.cache_folder) / filename\n",
    "\n",
    "    # if this file exists in the cache, return its full path\n",
    "    return filepath if filepath.is_file() else None\n",
    "\n",
    "\n",
    "def _retrieve_from_cache(url, check_remark=False):\n",
    "    \"\"\"\n",
    "    Retrieve a HTTP response JSON object from the cache, if it exists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        the URL of the request\n",
    "    check_remark : string\n",
    "        if True, only return filepath if cached response does not have a\n",
    "        remark key indicating a server warning\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response_json : dict\n",
    "        cached response for url if it exists in the cache, otherwise None\n",
    "    \"\"\"\n",
    "    # if the tool is configured to use the cache\n",
    "    if settings.use_cache:\n",
    "\n",
    "        # return cached response for this url if exists, otherwise return None\n",
    "        cache_filepath = _url_in_cache(url)\n",
    "        if cache_filepath is not None:\n",
    "            response_json = json.loads(cache_filepath.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "            # return None if check_remark is True and there is a server\n",
    "            # remark in the cached response\n",
    "            if check_remark and \"remark\" in response_json:\n",
    "                utils.log(f'Found remark, so ignoring cache file \"{cache_filepath}\"')\n",
    "                return None\n",
    "\n",
    "            utils.log(f'Retrieved response from cache file \"{cache_filepath}\"')\n",
    "            return response_json\n",
    "\n",
    "\n",
    "def _get_http_headers(user_agent=None, referer=None, accept_language=None):\n",
    "    \"\"\"\n",
    "    Update the default requests HTTP headers with OSMnx info.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_agent : string\n",
    "        the user agent string, if None will set with OSMnx default\n",
    "    referer : string\n",
    "        the referer string, if None will set with OSMnx default\n",
    "    accept_language : string\n",
    "        make accept-language explicit e.g. for consistent nominatim result\n",
    "        sorting\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    headers : dict\n",
    "    \"\"\"\n",
    "    if user_agent is None:\n",
    "        user_agent = settings.default_user_agent\n",
    "    if referer is None:\n",
    "        referer = settings.default_referer\n",
    "    if accept_language is None:\n",
    "        accept_language = settings.default_accept_language\n",
    "\n",
    "    headers = requests.utils.default_headers()\n",
    "    headers.update(\n",
    "        {\"User-Agent\": user_agent, \"referer\": referer, \"Accept-Language\": accept_language}\n",
    "    )\n",
    "    return headers\n",
    "\n",
    "\n",
    "def _config_dns(url):\n",
    "    \"\"\"\n",
    "    Force socket.getaddrinfo to use IP address instead of host.\n",
    "\n",
    "    Resolves the URL's domain to an IP address so that we use the same server\n",
    "    for both 1) checking the necessary pause duration and 2) sending the query\n",
    "    itself even if there is round-robin redirecting among multiple server\n",
    "    machines on the server-side. Mutates the getaddrinfo function so it uses\n",
    "    the same IP address everytime it finds the host name in the URL.\n",
    "\n",
    "    For example, the domain overpass-api.de just redirects to one of its\n",
    "    subdomains (currently z.overpass-api.de and lz4.overpass-api.de). So if we\n",
    "    check the status endpoint of overpass-api.de, we may see results for\n",
    "    subdomain z, but when we submit the query itself it gets redirected to\n",
    "    subdomain lz4. This could result in violating server lz4's slot management\n",
    "    timing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        the URL to consistently resolve the IP address of\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    host = urlparse(url).netloc.split(\":\")[0]\n",
    "    ip = socket.gethostbyname(host)\n",
    "\n",
    "    def _getaddrinfo(*args):\n",
    "        if args[0] == host:\n",
    "            utils.log(f\"Resolved {host} to {ip}\")\n",
    "            return _original_getaddrinfo(ip, *args[1:])\n",
    "        else:\n",
    "            return _original_getaddrinfo(*args)\n",
    "\n",
    "    socket.getaddrinfo = _getaddrinfo\n",
    "\n",
    "\n",
    "def _get_pause(base_endpoint, recursive_delay=5, default_duration=60):\n",
    "    \"\"\"\n",
    "    Get a pause duration from the Overpass API status endpoint.\n",
    "\n",
    "    Check the Overpass API status endpoint to determine how long to wait until\n",
    "    the next slot is available. You can disable this via the `ox.config`\n",
    "    function's `overpass_rate_limit` argument.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_endpoint : string\n",
    "        base Overpass API endpoint (without \"/status\" at the end)\n",
    "    recursive_delay : int\n",
    "        how long to wait between recursive calls if the server is currently\n",
    "        running a query\n",
    "    default_duration : int\n",
    "        if fatal error, fall back on returning this value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pause : int\n",
    "    \"\"\"\n",
    "    if not settings.overpass_rate_limit:  # pragma: no cover\n",
    "        # if overpass rate limiting is False, then there is zero pause\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        url = base_endpoint.rstrip(\"/\") + \"/status\"\n",
    "        response = requests.get(url, headers=_get_http_headers())\n",
    "        status = response.text.split(\"\\n\")[3]\n",
    "        status_first_token = status.split(\" \")[0]\n",
    "\n",
    "    except Exception:  # pragma: no cover\n",
    "        # if we cannot reach the status endpoint or parse its output, log an\n",
    "        # error and return default duration\n",
    "        sc = response.status_code\n",
    "        utils.log(f\"Unable to query {url} got status {sc}\", level=lg.ERROR)\n",
    "        return default_duration\n",
    "\n",
    "    try:\n",
    "        # if first token is numeric, it's how many slots you have available,\n",
    "        # no wait required\n",
    "        _ = int(status_first_token)  # number of available slots\n",
    "        pause = 0\n",
    "\n",
    "    except Exception:  # pragma: no cover\n",
    "        # if first token is 'Slot', it tells you when your slot will be free\n",
    "        if status_first_token == \"Slot\":\n",
    "            utc_time_str = status.split(\" \")[3]\n",
    "            utc_time = date_parser.parse(utc_time_str).replace(tzinfo=None)\n",
    "            pause = int(np.ceil((utc_time - dt.datetime.utcnow()).total_seconds()))\n",
    "            pause = max(pause, 1)\n",
    "\n",
    "        # if first token is 'Currently', it is currently running a query so\n",
    "        # check back in recursive_delay seconds\n",
    "        elif status_first_token == \"Currently\":\n",
    "            time.sleep(recursive_delay)\n",
    "            pause = _get_pause(base_endpoint)\n",
    "\n",
    "        # any other status is unrecognized: log error, return default duration\n",
    "        else:\n",
    "            utils.log(f'Unrecognized server status: \"{status}\"', level=lg.ERROR)\n",
    "            return default_duration\n",
    "\n",
    "    return pause\n",
    "\n",
    "\n",
    "def _make_overpass_settings():\n",
    "    \"\"\"\n",
    "    Make settings string to send in Overpass query.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "    \"\"\"\n",
    "    if settings.memory is None:\n",
    "        maxsize = \"\"\n",
    "    else:\n",
    "        maxsize = f\"[maxsize:{settings.memory}]\"\n",
    "    return settings.overpass_settings.format(timeout=settings.timeout, maxsize=maxsize)\n",
    "\n",
    "\n",
    "def _make_overpass_polygon_coord_strs(polygon):\n",
    "    \"\"\"\n",
    "    Subdivide query polygon and return list of coordinate strings.\n",
    "\n",
    "    Project to utm, divide polygon up into sub-polygons if area exceeds a\n",
    "    max size (in meters), project back to lat-lng, then get a list of\n",
    "    polygon(s) exterior coordinates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    polygon : shapely.geometry.Polygon or shapely.geometry.MultiPolygon\n",
    "        geographic boundaries to fetch the OSM geometries within\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    polygon_coord_strs : list\n",
    "        list of exterior coordinate strings for smaller sub-divided polygons\n",
    "    \"\"\"\n",
    "    geometry_proj, crs_proj = projection.project_geometry(polygon)\n",
    "    gpcs = utils_geo._consolidate_subdivide_geometry(geometry_proj)\n",
    "    geometry, _ = projection.project_geometry(gpcs, crs=crs_proj, to_latlong=True)\n",
    "    polygon_coord_strs = utils_geo._get_polygons_coordinates(geometry)\n",
    "    utils.log(f\"Requesting data within polygon from API in {len(polygon_coord_strs)} request(s)\")\n",
    "    return polygon_coord_strs\n",
    "\n",
    "\n",
    "def _create_overpass_query(polygon_coord_str, tags):\n",
    "    \"\"\"\n",
    "    Create an overpass query string based on passed tags.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    polygon_coord_str : list\n",
    "        list of lat lng coordinates\n",
    "    tags : dict\n",
    "        dict of tags used for finding elements in the selected area\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    query : string\n",
    "    \"\"\"\n",
    "    # create overpass settings string\n",
    "    overpass_settings = _make_overpass_settings()\n",
    "\n",
    "    # make sure every value in dict is bool, str, or list of str\n",
    "    error_msg = \"tags must be a dict with values of bool, str, or list of str\"\n",
    "    if not isinstance(tags, dict):  # pragma: no cover\n",
    "        raise TypeError(error_msg)\n",
    "\n",
    "    tags_dict = dict()\n",
    "    for key, value in tags.items():\n",
    "\n",
    "        if isinstance(value, bool):\n",
    "            tags_dict[key] = value\n",
    "\n",
    "        elif isinstance(value, str):\n",
    "            tags_dict[key] = [value]\n",
    "\n",
    "        elif isinstance(value, list):\n",
    "            if not all(isinstance(s, str) for s in value):  # pragma: no cover\n",
    "                raise TypeError(error_msg)\n",
    "            tags_dict[key] = value\n",
    "\n",
    "        else:  # pragma: no cover\n",
    "            raise TypeError(error_msg)\n",
    "\n",
    "    # convert the tags dict into a list of {tag:value} dicts\n",
    "    tags_list = []\n",
    "    for key, value in tags_dict.items():\n",
    "        if isinstance(value, bool):\n",
    "            tags_list.append({key: value})\n",
    "        else:\n",
    "            for value_item in value:\n",
    "                tags_list.append({key: value_item})\n",
    "\n",
    "    # add node/way/relation query components one at a time\n",
    "    components = []\n",
    "    for d in tags_list:\n",
    "        for key, value in d.items():\n",
    "\n",
    "            if isinstance(value, bool):\n",
    "                # if bool (ie, True) just pass the key, no value\n",
    "                tag_str = f\"['{key}'](poly:'{polygon_coord_str}');(._;>;);\"\n",
    "            else:\n",
    "                # otherwise, pass \"key\"=\"value\"\n",
    "                tag_str = f\"['{key}'='{value}'](poly:'{polygon_coord_str}');(._;>;);\"\n",
    "\n",
    "            for kind in (\"node\", \"way\", \"relation\"):\n",
    "                components.append(f\"({kind}{tag_str});\")\n",
    "\n",
    "    # finalize query and return\n",
    "    components = \"\".join(components)\n",
    "    query = f\"{overpass_settings};({components});out;\"\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "def _osm_network_download(polygon, network_type, custom_filter):\n",
    "    \"\"\"\n",
    "    Retrieve networked ways and nodes within boundary from the Overpass API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    polygon : shapely.geometry.Polygon or shapely.geometry.MultiPolygon\n",
    "        boundary to fetch the network ways/nodes within\n",
    "    network_type : string\n",
    "        what type of street network to get if custom_filter is None\n",
    "    custom_filter : string\n",
    "        a custom ways filter to be used instead of the network_type presets\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response_jsons : list\n",
    "        list of JSON responses from the Overpass server\n",
    "    \"\"\"\n",
    "    # create a filter to exclude certain kinds of ways based on the requested\n",
    "    # network_type, if provided, otherwise use custom_filter\n",
    "    if custom_filter is not None:\n",
    "        osm_filter = custom_filter\n",
    "    else:\n",
    "        osm_filter = _get_osm_filter(network_type)\n",
    "\n",
    "    response_jsons = []\n",
    "\n",
    "    # create overpass settings string\n",
    "    overpass_settings = _make_overpass_settings()\n",
    "\n",
    "    # subdivide query polygon to get list of sub-divided polygon coord strings\n",
    "    polygon_coord_strs = _make_overpass_polygon_coord_strs(polygon)\n",
    "\n",
    "    # pass each polygon exterior coordinates in the list to the API, one at a\n",
    "    # time. The '>' makes it recurse so we get ways and the ways' nodes.\n",
    "    for polygon_coord_str in polygon_coord_strs:\n",
    "        query_str = f\"{overpass_settings};(way{osm_filter}(poly:'{polygon_coord_str}');>;);out;\"\n",
    "        response_json = overpass_request(data={\"data\": query_str})\n",
    "        response_jsons.append(response_json)\n",
    "    utils.log(\n",
    "        f\"Got all network data within polygon from API in {len(polygon_coord_strs)} request(s)\"\n",
    "    )\n",
    "\n",
    "    if settings.cache_only_mode:  # pragma: no cover\n",
    "        raise CacheOnlyModeInterrupt(\"settings.cache_only_mode=True\")\n",
    "\n",
    "    return response_jsons\n",
    "\n",
    "\n",
    "def _osm_geometries_download(polygon, tags):\n",
    "    \"\"\"\n",
    "    Retrieve non-networked elements within boundary from the Overpass API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    polygon : shapely.geometry.Polygon\n",
    "        boundaries to fetch elements within\n",
    "    tags : dict\n",
    "        dict of tags used for finding elements in the selected area\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response_jsons : list\n",
    "        list of JSON responses from the Overpass server\n",
    "    \"\"\"\n",
    "    response_jsons = []\n",
    "\n",
    "    # subdivide query polygon to get list of sub-divided polygon coord strings\n",
    "    polygon_coord_strs = _make_overpass_polygon_coord_strs(polygon)\n",
    "\n",
    "    # pass exterior coordinates of each polygon in list to API, one at a time\n",
    "    for polygon_coord_str in polygon_coord_strs:\n",
    "        query_str = _create_overpass_query(polygon_coord_str, tags)\n",
    "        response_json = overpass_request(data={\"data\": query_str})\n",
    "        response_jsons.append(response_json)\n",
    "\n",
    "    utils.log(\n",
    "        f\"Got all geometries data within polygon from API in {len(polygon_coord_strs)} request(s)\"\n",
    "    )\n",
    "\n",
    "    return response_jsons\n",
    "\n",
    "\n",
    "def _osm_place_download(query, by_osmid=False, limit=1, polygon_geojson=1):\n",
    "    \"\"\"\n",
    "    Retrieve a place from the Nominatim API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : string or dict\n",
    "        query string or structured query dict\n",
    "    by_osmid : bool\n",
    "        if True, handle query as an OSM ID for lookup rather than text search\n",
    "    limit : int\n",
    "        max number of results to return\n",
    "    polygon_geojson : int\n",
    "        retrieve the place's geometry from the API, 0=no, 1=yes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response_json : dict\n",
    "        JSON response from the Nominatim server\n",
    "    \"\"\"\n",
    "    # define the parameters\n",
    "    params = OrderedDict()\n",
    "    params[\"format\"] = \"json\"\n",
    "    params[\"polygon_geojson\"] = polygon_geojson\n",
    "\n",
    "    if by_osmid:\n",
    "        # if querying by OSM ID, use the lookup endpoint\n",
    "        request_type = \"lookup\"\n",
    "        params[\"osm_ids\"] = query\n",
    "\n",
    "    else:\n",
    "        # if not querying by OSM ID, use the search endpoint\n",
    "        request_type = \"search\"\n",
    "\n",
    "        # prevent OSM from deduping so we get precise number of results\n",
    "        params[\"dedupe\"] = 0\n",
    "        params[\"limit\"] = limit\n",
    "\n",
    "        if isinstance(query, str):\n",
    "            params[\"q\"] = query\n",
    "        elif isinstance(query, dict):\n",
    "            # add query keys in alphabetical order so URL is the same string\n",
    "            # each time, for caching purposes\n",
    "            for key in sorted(query):\n",
    "                params[key] = query[key]\n",
    "        else:  # pragma: no cover\n",
    "            raise TypeError(\"query must be a dict or a string\")\n",
    "\n",
    "    # request the URL, return the JSON\n",
    "    response_json = nominatim_request(params=params, request_type=request_type)\n",
    "    return response_json\n",
    "\n",
    "\n",
    "def nominatim_request(params, request_type=\"search\", pause=1, error_pause=60):\n",
    "    \"\"\"\n",
    "    Send a HTTP GET request to the Nominatim API and return JSON response.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : OrderedDict\n",
    "        key-value pairs of parameters\n",
    "    request_type : string {\"search\", \"reverse\", \"lookup\"}\n",
    "        which Nominatim API endpoint to query\n",
    "    pause : int\n",
    "        how long to pause before request, in seconds. per the nominatim usage\n",
    "        policy: \"an absolute maximum of 1 request per second\" is allowed\n",
    "    error_pause : int\n",
    "        how long to pause in seconds before re-trying request if error\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response_json : dict\n",
    "    \"\"\"\n",
    "    if request_type not in {\"search\", \"reverse\", \"lookup\"}:  # pragma: no cover\n",
    "        raise ValueError('Nominatim request_type must be \"search\", \"reverse\", or \"lookup\"')\n",
    "\n",
    "    # prepare Nominatim API URL and see if request already exists in cache\n",
    "    url = settings.nominatim_endpoint.rstrip(\"/\") + \"/\" + request_type\n",
    "    prepared_url = requests.Request(\"GET\", url, params=params).prepare().url\n",
    "    cached_response_json = _retrieve_from_cache(prepared_url)\n",
    "\n",
    "    if settings.nominatim_key:\n",
    "        params[\"key\"] = settings.nominatim_key\n",
    "\n",
    "    if cached_response_json is not None:\n",
    "        # found response in the cache, return it instead of calling server\n",
    "        return cached_response_json\n",
    "\n",
    "    else:\n",
    "        # if this URL is not already in the cache, pause, then request it\n",
    "        utils.log(f\"Pausing {pause} seconds before making HTTP GET request\")\n",
    "        time.sleep(pause)\n",
    "\n",
    "        # transmit the HTTP GET request\n",
    "        utils.log(f\"Get {prepared_url} with timeout={settings.timeout}\")\n",
    "        headers = _get_http_headers()\n",
    "        response = requests.get(url, params=params, timeout=settings.timeout, headers=headers)\n",
    "        sc = response.status_code\n",
    "\n",
    "        # log the response size and domain\n",
    "        size_kb = len(response.content) / 1000\n",
    "        domain = re.findall(r\"(?s)//(.*?)/\", url)[0]\n",
    "        utils.log(f\"Downloaded {size_kb:,.1f}kB from {domain}\")\n",
    "\n",
    "        try:\n",
    "            response_json = response.json()\n",
    "\n",
    "        except Exception:  # pragma: no cover\n",
    "            if sc in {429, 504}:\n",
    "                # 429 is 'too many requests' and 504 is 'gateway timeout' from\n",
    "                # server overload: handle these by pausing then recursively\n",
    "                # re-trying until we get a valid response from the server\n",
    "                utils.log(f\"{domain} returned {sc}: retry in {error_pause} secs\", level=lg.WARNING)\n",
    "                time.sleep(error_pause)\n",
    "                response_json = nominatim_request(params, request_type, pause, error_pause)\n",
    "\n",
    "            else:\n",
    "                # else, this was an unhandled status code, throw an exception\n",
    "                utils.log(f\"{domain} returned {sc}\", level=lg.ERROR)\n",
    "                raise Exception(f\"Server returned:\\n{response} {response.reason}\\n{response.text}\")\n",
    "\n",
    "        _save_to_cache(prepared_url, response_json, sc)\n",
    "        return response_json\n",
    "\n",
    "\n",
    "def overpass_request(data, pause=None, error_pause=60):\n",
    "    \"\"\"\n",
    "    Send a HTTP POST request to the Overpass API and return JSON response.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : OrderedDict\n",
    "        key-value pairs of parameters\n",
    "    pause : int\n",
    "        how long to pause in seconds before request, if None, will query API\n",
    "        status endpoint to find when next slot is available\n",
    "    error_pause : int\n",
    "        how long to pause in seconds (in addition to `pause`) before re-trying\n",
    "        request if error\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response_json : dict\n",
    "    \"\"\"\n",
    "    base_endpoint = settings.overpass_endpoint\n",
    "\n",
    "    # resolve url to same IP even if there is server round-robin redirecting\n",
    "    _config_dns(base_endpoint)\n",
    "\n",
    "    # define the Overpass API URL, then construct a GET-style URL as a string to\n",
    "    # hash to look up/save to cache\n",
    "    url = base_endpoint.rstrip(\"/\") + \"/interpreter\"\n",
    "    prepared_url = requests.Request(\"GET\", url, params=data).prepare().url\n",
    "    cached_response_json = _retrieve_from_cache(prepared_url, check_remark=True)\n",
    "\n",
    "    if cached_response_json is not None:\n",
    "        # found response in the cache, return it instead of calling server\n",
    "        return cached_response_json\n",
    "\n",
    "    else:\n",
    "        # if this URL is not already in the cache, pause, then request it\n",
    "        if pause is None:\n",
    "            this_pause = _get_pause(base_endpoint)\n",
    "        utils.log(f\"Pausing {this_pause} seconds before making HTTP POST request\")\n",
    "        time.sleep(this_pause)\n",
    "\n",
    "        # transmit the HTTP POST request\n",
    "        utils.log(f\"Post {prepared_url} with timeout={settings.timeout}\")\n",
    "        headers = _get_http_headers()\n",
    "        response = requests.post(url, data=data, timeout=settings.timeout, headers=headers)\n",
    "        sc = response.status_code\n",
    "\n",
    "        # log the response size and domain\n",
    "        size_kb = len(response.content) / 1000\n",
    "        domain = re.findall(r\"(?s)//(.*?)/\", url)[0]\n",
    "        utils.log(f\"Downloaded {size_kb:,.1f}kB from {domain}\")\n",
    "\n",
    "        try:\n",
    "            response_json = response.json()\n",
    "            if \"remark\" in response_json:\n",
    "                utils.log(f'Server remark: \"{response_json[\"remark\"]}\"', level=lg.WARNING)\n",
    "\n",
    "        except Exception:  # pragma: no cover\n",
    "            if sc in {429, 504}:\n",
    "                # 429 is 'too many requests' and 504 is 'gateway timeout' from\n",
    "                # server overload: handle these by pausing then recursively\n",
    "                # re-trying until we get a valid response from the server\n",
    "                this_pause = error_pause + _get_pause(base_endpoint)\n",
    "                utils.log(f\"{domain} returned {sc}: retry in {this_pause} secs\", level=lg.WARNING)\n",
    "                time.sleep(this_pause)\n",
    "                response_json = overpass_request(data, pause, error_pause)\n",
    "\n",
    "            else:\n",
    "                # else, this was an unhandled status code, throw an exception\n",
    "                utils.log(f\"{domain} returned {sc}\", level=lg.ERROR)\n",
    "                raise Exception(f\"Server returned\\n{response} {response.reason}\\n{response.text}\")\n",
    "\n",
    "        _save_to_cache(prepared_url, response_json, sc)\n",
    "        return response_json\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
